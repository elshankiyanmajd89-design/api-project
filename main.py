import streamlit as st
import os
from openai import OpenAI
import fitz
from pdf2image import convert_from_path
import pytesseract

# ---------------- CONFIG ----------------
MODEL = "gpt-4o-mini"
MAX_CONTEXT_TOKENS = 120000  # Ú©Ù…ØªØ± Ø§Ø² 128k
CHUNK_SIZE = 1200
CHUNK_OVERLAP = 100
MAX_CHUNKS_PER_STAGE = 1
MAX_CHUNKS_DIRECT = 5
SUMMARY_SIZE = 1500

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

API_KEY = st.secrets["general"]["GAPGPT_API_KEY"]
client = OpenAI(base_url="https://api.gapgpt.app/v1", api_key=API_KEY)

# ---------------- UTILS ----------------
def extract_text_from_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    return "".join([page.get_text() for page in doc])

def extract_text_from_scanned_pdf(file_path):
    images = convert_from_path(file_path)
    text = ""
    for img in images:
        text += pytesseract.image_to_string(img, lang='fas+eng') + "\n"
    return text

def load_local_sources():
    sources = {}
    for folder in ["sources", "books"]:
        if not os.path.exists(folder):
            continue
        for fname in os.listdir(folder):
            path = os.path.join(folder, fname)
            try:
                if fname.endswith(".txt"):
                    with open(path, "r", encoding="utf-8") as f:
                        text = f.read()
                elif fname.endswith(".pdf"):
                    if folder == "books":
                        with open(path, "rb") as f:
                            text = extract_text_from_pdf(f)
                    else:
                        text = extract_text_from_scanned_pdf(path)
                else:
                    continue
                sources[f"{folder}/{fname}"] = text
            except Exception as e:
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {path}: {e}")
    return sources

def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    words = text.split()
    chunks, current = [], []
    for w in words:
        current.append(w)
        if len(current) >= chunk_size:
            chunks.append(" ".join(current))
            current = current[-overlap:]
    if current:
        chunks.append(" ".join(current))
    return chunks

def ask_gapgpt(prompt, max_tokens=SUMMARY_SIZE):
    try:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful legal assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=max_tokens
        )
        return resp.choices[0].message.content
    except Exception as e:
        st.error(f"Ù…Ø´Ú©Ù„ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ GapGPT API: {e}")
        return ""

def summarize_chunks(chunks, question):
    summaries = []
    for i, chunk in enumerate(chunks[:MAX_CHUNKS_PER_STAGE]):
        if st.session_state.stop_flag:
            st.warning("Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯!")
            break
        st.info(f"Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ chunk {i+1}/{len(chunks[:MAX_CHUNKS_PER_STAGE])}")
        prompt = f"Ø§ÛŒÙ† Ù…ØªÙ† Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø± Ø®Ù„Ø§ØµÙ‡ Ú©Ù†:\n\n{chunk}\n\nØ³ÙˆØ§Ù„: {question}\nØ®Ù„Ø§ØµÙ‡:"
        summary = ask_gapgpt(prompt)
        summaries.append(summary)
    return summaries

# ---------------- UI ----------------
st.set_page_config(page_title="Ù…Ø´Ø§ÙˆØ± ÙˆÚ©ÛŒÙ„", layout="centered")
st.title("ğŸ“„ Ù…Ø´Ø§ÙˆØ± Ø­Ù‚ÙˆÙ‚ÛŒ")

# Reset stop flag at the start
if "stop_flag" not in st.session_state:
    st.session_state.stop_flag = False

# ON/OFF Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ù„ÛŒ
use_local = st.checkbox("Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ù„ÛŒ Ø±ÙˆÛŒ Ù„Ù¾â€ŒØªØ§Ù¾")

# Ø¢Ù¾Ù„ÙˆØ¯ PDF Ø§Ø®ØªÛŒØ§Ø±ÛŒ
uploaded_file = st.file_uploader("Upload PDF (optional)", type=["pdf"])
question = st.text_area("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", placeholder="Ù…Ø«Ù„Ø§Ù‹: Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡ / Ø®Ù„Ø§ØµÙ‡ Ú©Ù†")

# Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
col1, col2, col3 = st.columns(3)
with col1:
    direct_api_btn = st.button("Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² GapGPT API")
with col2:
    summary_btn = st.button("Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ")
with col3:
    stop_btn = st.button("Stop")
    if stop_btn:
        st.session_state.stop_flag = True
        st.warning("Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙˆÙ‚Ù Ø´Ø¯!")

# ---------------- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø¨Ø¹ ----------------
sources = load_local_sources() if use_local else {}
selected_texts = list(sources.values())

context_chunks = []

# ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡
if uploaded_file:
    text = extract_text_from_pdf(uploaded_file)
    if len(text.strip()) < 50:  # Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø³Ú©Ù†
        tmp_path = f"temp_{uploaded_file.name}"
        with open(tmp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        text = extract_text_from_scanned_pdf(tmp_path)
        os.remove(tmp_path)
    context_chunks.extend(chunk_text(text))

# Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ù„ÛŒ
if use_local:
    for t in selected_texts:
        context_chunks.extend(chunk_text(t))

# ---------------- Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… ----------------
if direct_api_btn and question.strip():
    st.session_state.stop_flag = False  # Reset stop flag
    st.info("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø¨Ø¯ÙˆÙ† Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ...")
    if context_chunks:
        total_chunks = min(len(context_chunks), MAX_CHUNKS_DIRECT)
        combined_text = "\n".join(context_chunks[:total_chunks])
        prompt = f"{combined_text}\n\nØ³ÙˆØ§Ù„: {question}\nØ¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡:"
    else:
        prompt = question  # ÙÙ‚Ø· Ø®ÙˆØ¯ Ø³ÙˆØ§Ù„ Ø¨Ø¯ÙˆÙ† Ù…Ù†Ø§Ø¨Ø¹
    answer = ask_gapgpt(prompt)
    st.subheader("ğŸ“Œ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…")
    st.write(answer)

# ---------------- Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ----------------
if summary_btn and question.strip():
    st.session_state.stop_flag = False  # Reset stop flag
    st.info(f"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ chunks Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´: {len(context_chunks[:MAX_CHUNKS_PER_STAGE])}")
    if context_chunks:
        summaries = summarize_chunks(context_chunks, question)
        final_answer = "\n".join(summaries)
        st.subheader("ğŸ“Œ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ")
        st.write(final_answer)
    else:
        st.warning("Ù‡ÛŒÚ† Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")

st.caption("âš ï¸ Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ø´Ø§ÙˆØ±Ù‡ Ø±Ø³Ù…ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ Ù†ÛŒØ³Øª.")
